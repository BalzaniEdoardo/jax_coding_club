{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc099f4-b49f-418c-8c72-5e5622618363",
   "metadata": {},
   "source": [
    "# Intdoduction to JAX\n",
    "\n",
    "At its core, JAX provides a way to transform functions using NumPy to get automatic differentiation, GPU/TPU acceleration, and much more.\n",
    "\n",
    "## JAX and Numpy\n",
    "\n",
    "Using JAX feels very much like using NumPy. The majority of your NumPy knowledge is directly transferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e2462ea-955b-44b8-a6dc-fe44042955e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basic examples: array creation, reshaping, and slicing in both JAX and NumPy as side-by-side code snippets.\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Examples of array definition:\n",
    "# Numpy\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "numpy_ones = np.ones(10)\n",
    "\n",
    "# JAX\n",
    "jax_array = jnp.array([[1, 2], [3, 4]])\n",
    "jax_ones = jax.numpy.ones(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1202727-543d-4eb2-9045-cb452e433662",
   "metadata": {},
   "source": [
    "However, JAX arrays are diffenrent in several important ways: \n",
    "\n",
    "- **Immutability:** Once an array is created, its contents cannot be changed, the same way python `tuple` works. \n",
    "- **Device Memory:** JAX arrays can live in device memory (like on a GPU), while NumPy arrays live in host memory.\n",
    "- **Lazy Evaluation:** Unlike NumPy which evaluates operations immediately, JAX can be lazy in its evaluation, waiting to execute operations until the result is actually needed. This is especially true when using some of JAX's transformations. (Hard to showcase with a simple example).\n",
    "- **Advanced Features:** JAX arrays operations can be auto-differentiated and parallelized (we touch on this later).\n",
    "\n",
    "## Immutability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91addb37-ab41-42f2-af58-6f382f4a2fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exeption raised: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    }
   ],
   "source": [
    "# Array immutability\n",
    "\n",
    "# Item assignment works for numpy arrays.\n",
    "numpy_array[1, 1] = 0\n",
    "\n",
    "# While in JAX it raises an TypeError.\n",
    "try:\n",
    "    jax_array[1, 1] = 0\n",
    "except TypeError as err:\n",
    "    print(f\"Exeption raised: {err}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317be93-2a6d-40cc-868b-7c2beb322b67",
   "metadata": {},
   "source": [
    "## Device Memory\n",
    "\n",
    "This is how you can display the device in which the array is on, and send it to another device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38689f57-79d5-4a58-a49d-d7dc0c2bdab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array device: TFRT_CPU_0\n",
      "List available devices [CpuDevice(id=0)]\n",
      "Array device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "# Display the device the array is on\n",
    "print(\"Array device:\", jax_array.device_buffer.device())\n",
    "\n",
    "# List all available devices\n",
    "print(\"List available devices\", jax.devices())\n",
    "\n",
    "# Send array to a desired device\n",
    "jax_array = jax.device_put(jax_array, jax.devices(\"cpu\")[0])\n",
    "print(\"Array device:\", jax_array.device_buffer.device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94681d7-5416-400c-b1d8-5026bda302d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPU-acceleration \n",
    "As a simple use case, we can see that on large matrices, JAX's GPU acceleration can be noticeably faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "badc7652-8ad2-4043-b592-bc08780bde60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 s ± 40.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "305 ms ± 7.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Create large matrices\n",
    "large_numpy_matrix = np.random.rand(5000, 5000)\n",
    "large_jax_matrix = jnp.array(large_numpy_matrix)\n",
    "\n",
    "# Time matrix multiplication in NumPy\n",
    "%timeit np.dot(large_numpy_matrix, large_numpy_matrix)\n",
    "\n",
    "# Time matrix multiplication in JAX\n",
    "%timeit jnp.dot(large_jax_matrix, large_jax_matrix).block_until_ready() # The block_until_ready ensures we wait for the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc55bd6-63be-4059-a149-31cdd6e9439a",
   "metadata": {},
   "source": [
    "## Just-In-Time Compilation with `jit`\n",
    "\n",
    "### What is JIT Compilation?\n",
    "\n",
    "Just-In-Time (JIT) Compilation is a technique where the code is compiled during execution rather than before execution starts. This allows certain optimizations to be made that can't be achieved with traditional ahead-of-time compilation.\n",
    "\n",
    "For those of you familiar with `numba`, JAX's `jit` serves a similar purpose: it takes a Python function and optimizes it for faster execution. However, while `numba` is designed primarily for speeding up CPU-bound Python code, JAX's `jit` can target both CPU and accelerators like GPUs and TPUs.\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Here's how to use `jit` in JAX:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ada960c8-6322-4239-aa42-2f1cec9570b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45464867 -0.37840125 -0.13970774]\n",
      "JIT compiled with decorators: <PjitFunction of <function decorated_function at 0x11f841f80>>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def slow_function(x):\n",
    "    return jnp.sin(x) * jnp.cos(x)\n",
    "\n",
    "compiled_function = jax.jit(slow_function)\n",
    "\n",
    "# Now, fast_function will run the optimized version\n",
    "result = compiled_function(jnp.array([1.0, 2.0, 3.0]))\n",
    "print(result)\n",
    "\n",
    "# Alternatively you can use decorators\n",
    "@jax.jit\n",
    "def decorated_function(x):\n",
    "    return jnp.sin(x) * jnp.cos(x)\n",
    "\n",
    "print(f\"JIT compiled with decorators: {decorated_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64423472-ad71-4a20-b2c9-2b6610ee33ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tracers\n",
    "When you jit a function, JAX introduces what are known as \"tracers\" into the function. Tracers can be thought of as symbolic representations of your input, and they help JAX figure out how to optimize your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcbcc886-e7e2-40ad-aac8-0b5b23e52650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[3]. let\n",
      "    b:f32[3] = sin a\n",
      "    c:f32[3] = cos a\n",
      "    d:f32[3] = mul b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "from jax import make_jaxpr\n",
    "\n",
    "# Let's see the jax expression of our function\n",
    "print(make_jaxpr(slow_function)(jnp.array([1.0, 2.0, 3.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab800a1b-5a50-4334-bce6-3db44d801abc",
   "metadata": {},
   "source": [
    "The output is a JAX expression, a symbolic representation of your function's operations. This is what JAX uses internally to optimize your code.\n",
    "\n",
    "### Debugging JIT-compiled Code\n",
    "\n",
    "Certain functions, such as `jax.lax.scan`, undergo JIT compilation by default. This can make debugging challenging since, during the process, you'll be able to inspect tracers rather than the computed values. To facilitate debugging, you can deactivate the jit compilation as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42b6ce62-0bb9-4a08-b2fb-ab3a4a8815c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT enabled\n",
      "Iteration 1:\n",
      "Inside jitted function: Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Iteration 2:\n",
      "Iteration 3:\n",
      "\n",
      "JIT disabled\n",
      "Iteration 1:\n",
      "Inside jitted function: [1. 2. 3.]\n",
      "Iteration 2:\n",
      "Inside jitted function: [1. 2. 3.]\n",
      "Iteration 3:\n",
      "Inside jitted function: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@jax.jit\n",
    "def jitted_function(x):\n",
    "    print(\"Inside jitted function:\", x)\n",
    "\n",
    "# Call the function with some data\n",
    "# The print inside the function will be executed only once, at compilation time\n",
    "print(\"JIT enabled\")\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "for i in range(3):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    jitted_function(x)\n",
    "\n",
    "print(\"\\nJIT disabled\")\n",
    "with jax.disable_jit():\n",
    "    for i in range(3):\n",
    "        print(f\"Iteration {i+1}:\")\n",
    "        jitted_function(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5424ce3-86c2-4844-a046-896aa32f3d2d",
   "metadata": {},
   "source": [
    "## Autodifferentiation with JAX\n",
    "Autodifferentiation, often referred to as \"autograd\" or simply \"AD\", is the ability to compute gradients of functions automatically. JAX provides this capability seamlessly. Let's take a look at how this works.\n",
    "\n",
    "### Toy Example 1: Linear Regression\n",
    "We can start off with a standard example, fitting a linear regression by gradient descent on a Mean Squared Error loss.\n",
    "\n",
    "Let's consider a simple linear regression model: $f(x)=mx+b$, and $y=f(x) + \\text{noise}$, where we'll optimize for the slope $m$ and the intercept $b$. We'll use the Mean Squared Error as our loss function.\n",
    "\n",
    "First, let's generate some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24f1ba9b-9a1a-4f92-b8bc-79639da98743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "# Generating some sample data\n",
    "true_m = 2.5\n",
    "true_b = -1.0\n",
    "x_data = jnp.linspace(-10, 10, 1000)\n",
    "y_data = true_m * x_data + true_b + jax.random.normal(jax.random.PRNGKey(123), shape=x_data.shape) * 2.0  # Adding some noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d655e01-2c39-49db-99b9-7b4b1d365772",
   "metadata": {},
   "source": [
    "Let's define the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d293e70-c0d7-49cc-b339-e7a00486bf65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "def mse_loss(params, x, y):\n",
    "    m, b = params\n",
    "    y_pred = linear_model(x, m, b)\n",
    "    return jnp.mean((y_pred - y)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dcb3eb-34a5-4d92-8acb-e1528978ddeb",
   "metadata": {},
   "source": [
    "We can now compute the gradient via AD, by passing the lossn through the `jax.grad` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49d1a87d-4c47-4c30-b99d-fae89a623b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient = jax.grad(mse_loss, argnums=0)  # 0 refers to the first argument of mse_loss, i.e., the params tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f2b7d-66a4-400a-8c68-9e07ef273e5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "One can perform gradien decent or any other gradient based optimization on the loss. \n",
    "\n",
    "There are many excellent libraries for optimization in the `jax` framwork, for example [`JAXopt`](https://jaxopt.github.io/stable/) for general purpose optimizaiton or [`flax`](https://flax.readthedocs.io/en/latest/) which is specific for neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e962562-130e-4621-a3a7-bb8e653e4210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92: m = 2.4941, b = -0.7404, Loss = 4.1020\n",
      "Iteration 93: m = 2.4941, b = -0.7432, Loss = 4.1012\n",
      "Iteration 94: m = 2.4941, b = -0.7458, Loss = 4.1005\n",
      "Iteration 95: m = 2.4941, b = -0.7485, Loss = 4.0998\n",
      "Iteration 96: m = 2.4941, b = -0.7510, Loss = 4.0992\n",
      "Iteration 97: m = 2.4941, b = -0.7536, Loss = 4.0985\n",
      "Iteration 98: m = 2.4941, b = -0.7560, Loss = 4.0979\n",
      "Iteration 99: m = 2.4941, b = -0.7585, Loss = 4.0974\n",
      "Iteration 100: m = 2.4941, b = -0.7608, Loss = 4.0968\n",
      "\n",
      "Optimization results,  m: 2.4941420555114746, b: -0.7608301043510437\n",
      "JAXopt results,        m: 2.4939701557159424, b: -0.8771588802337646\n"
     ]
    }
   ],
   "source": [
    "import jaxopt\n",
    "\n",
    "# Gradient Descent\n",
    "learning_rate = 0.01\n",
    "params = jnp.array([0.0, 0.0])  # Initial values for m and b\n",
    "\n",
    "for i in range(100):  # 100 gradient descent updates\n",
    "    grad_params = gradient(params, x_data, y_data)\n",
    "    params -= learning_rate * grad_params\n",
    "    if i > 90:\n",
    "        print(f\"Iteration {i+1}: m = {params[0]:.4f}, b = {params[1]:.4f}, Loss = {mse_loss(params, x_data, y_data):.4f}\")\n",
    "print(f\"\\nOptimization results,  m: {params[0]}, b: {params[1]}\")\n",
    "      \n",
    "# Gradient Descent via JAXopt\n",
    "solver = jaxopt.GradientDescent(fun=mse_loss, tol=10**-6)\n",
    "params, state = solver.run(init_params=(1., 1.), x=x_data, y=y_data)\n",
    "print(f\"JAXopt results,        m: {params[0]}, b: {params[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a647b5e-ec32-4177-952b-5e62fafff3a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Toy Example 2: Regression with a Two-layer Neural Network\n",
    "Now, let's add a two feed-forward layer with a ReLU activation. \n",
    "\n",
    "This will transform our model from a simple linear regression to a slighty complex neural network. The beauty of autodiff, is that this can be handled in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "08cfc658-84c9-412b-bdd3-5d50b427373d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 212.3490\n",
      "Iteration 2: Loss = 212.1717\n",
      "Iteration 3: Loss = 212.0583\n",
      "Iteration 4: Loss = 211.9857\n",
      "Iteration 5: Loss = 211.9392\n",
      "Iteration 6: Loss = 211.9095\n",
      "Iteration 7: Loss = 211.8904\n",
      "Iteration 8: Loss = 211.8782\n",
      "Iteration 9: Loss = 211.8704\n",
      "Iteration 10: Loss = 211.8654\n"
     ]
    }
   ],
   "source": [
    "def two_layer_nn_model(x, w1, b1, w2, b2, w_out, b_out):\n",
    "    hidden1 = jax.nn.relu(jnp.dot(x, w1) + b1)  # First ReLU activation\n",
    "    hidden2 = jax.nn.relu(jnp.dot(hidden1, w2) + b2)  # Second ReLU activation\n",
    "    return jnp.dot(hidden2, w_out) + b_out\n",
    "\n",
    "def two_layer_nn_mse_loss(params, x, y):\n",
    "    w1, b1, w2, b2, w_out, b_out = params\n",
    "    y_pred = two_layer_nn_model(x, w1, b1, w2, b2, w_out, b_out)\n",
    "    return jnp.mean((y_pred - y)**2)\n",
    "\n",
    "gradient_two_layer_nn = jax.grad(two_layer_nn_mse_loss, argnums=0)\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "params_two_layer_nn = (jnp.zeros((1, 10)), jnp.zeros(10), jnp.zeros((10, 10)), jnp.zeros(10), jnp.zeros(10), 0.0)  # Initialize weights and biases\n",
    "\n",
    "for i in range(100):\n",
    "    grad_params_two_layer_nn = gradient_two_layer_nn(params_two_layer_nn, x_data[:, None], y_data)  # x_data[:, None] reshapes x_data for matrix multiplication\n",
    "    params_two_layer_nn = tuple(param - learning_rate * grad_param for param, grad_param in zip(params_two_layer_nn, grad_params_two_layer_nn))\n",
    "    if i < 10:\n",
    "        print(f\"Iteration {i+1}: Loss = {two_layer_nn_mse_loss(params_two_layer_nn, x_data[:, None], y_data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee40a1d-bcd6-4c3d-9701-8a767f6d9f40",
   "metadata": {},
   "source": [
    "## Vectorization and Parallelization in JAX: `vmap` and `pmap`\n",
    "\n",
    "Parallelization is the process of performing multiple computations simultaneously. JAX offers two primary mechanisms for parallelizing code:\n",
    "\n",
    "1. Vectorization with **`vmap`**: This is a way to transparently turn a function that operates on single data points into one that operates on batches of data points.\n",
    "2. Parallelization across devices with **`pmap`**: It allows you to distribute computations over multiple accelerators.\n",
    "\n",
    "### Vectorization with vmap\n",
    "\n",
    "Suppose you have a function that computes the square of a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ef32e29-ce52-48df-b3f4-d833c899b9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2520c4-ab38-4125-b1d6-7e3f0c863053",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you want to compute the square of a batch of numbers, one way is to use a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e1c9d6c0-3deb-42bb-b44b-92a33b468850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "squared = jnp.array([square(x) for x in xs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c01a4-d332-44cd-8845-a816cef2d61a",
   "metadata": {},
   "source": [
    "With **`vmap`**, you can vectorize the square function so it can process the entire batch at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d19c211-951d-4f1c-b624-8d7c4b50ca0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batched_square = jax.vmap(square)\n",
    "squared = batched_square(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed1cf6-5723-461f-a665-cdc3222889ab",
   "metadata": {},
   "source": [
    "And parallelizaiton can be done on mutlidimensional arrays, specfing the input and output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "847811b2-c1e2-42c0-b66c-140860b22c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare outputs:\n",
      "vmap:\n",
      " [[ 7 10]\n",
      " [15 22]] \n",
      "regular dot product:\n",
      " [[ 7 10]\n",
      " [15 22]]\n",
      "\n",
      "\n",
      "Transpose outputs:\n",
      " [[ 7 15]\n",
      " [10 22]]\n"
     ]
    }
   ],
   "source": [
    "# Vector doct product\n",
    "vector_vector = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -> []\n",
    "\n",
    "# Using vmap, upgrade vv to handle matrix-vector multiplication. \n",
    "# We parallelize across the rows of the matrix (axis 0) and broadcast the vector.\n",
    "matrix_vector = jax.vmap(vector_vector, in_axes=(0, None), out_axes=0)      #  ([b,a], [a]) -> [b]      (b is the mapped axis)\n",
    "\n",
    "# Apply vmap again to upgrade mv to handle matrix-matrix multiplication. \n",
    "# This time we parallelize across the columns of the second matrix (axis 1).\n",
    "matrix_matrix = jax.vmap(matrix_vector, in_axes=(None, 1), out_axes=1)      #  ([b,a], [a,c]) -> [b,c]  (c is the mapped axis)\n",
    "\n",
    "print(\"Compare outputs:\\nvmap:\\n\", mm(jax_array, jax_array), \"\\nregular dot product:\\n\", jnp.dot(jax_array, jax_array))\n",
    "\n",
    "# The out_axes argument specifies where to store the parallelized axis in the output. \n",
    "# By setting it to 0 in the next function, we effectively transpose the output of the matrix-matrix multiplication.\n",
    "matrix_matrix_transpose = jax.vmap(matrix_vector, in_axes=(None, 1), out_axes=0)      #  ([b,a], [a,c]) -> [c,b]  (c is the mapped axis)\n",
    "print(\"\\n\\nTranspose outputs:\\n\", mm_transpose(jax_array, jax_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179764d-acf4-4ed4-9fc1-6fe181c8875c",
   "metadata": {},
   "source": [
    "### Parallelization across devices with **`pmap`**\n",
    "`pmap` (parallel map) is a parallelized version of `vmap` and is meant to distribute computations across multiple devices, typically multiple GPUs or TPUs. While vmap vectorizes computations, pmap physically runs them in parallel across devices. Here's a way to build on top of the matrix multiplication example to introduce pmap.\n",
    "1. **Setup**\n",
    "Before using pmap, you need to check the available devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "890fd8a1-cecf-4773-9d2e-749938d99a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "devices = jax.devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdc5a0-0cd1-4b42-a94b-d65b84ba66e4",
   "metadata": {},
   "source": [
    "**NOTE** You should take the result with a grain of salt. In real-world scenarios, GPU computation is often magnitudes faster than CPU for tasks like matrix multiplication. Plus, the data transfer time between the host (CPU) and device (GPU) can be a significant overhead. So, while it's an interesting demonstration, this setup might not be practically efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fcbfc-705f-4f30-ade0-7dd9bac7ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two arrays of shape (2, 10, 10), where the leading dimension 2 is for the two devices.\n",
    "x_sharded = jax.random.normal(jax.random.PRNGKey(123), shape=(2, 10, 10))\n",
    "y_sharded = jax.random.normal(jax.random.PRNGKey(123), shape=(2, 10, 10))\n",
    "\n",
    "# This will perform matrix multiplication in parallel on both devices.\n",
    "pmap_multiply= pmap(matrix_matrix, devices = devices[:2])\n",
    "result = pmap_multiply(x_sharded, y_sharded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdbf2a-8eba-4083-9ff8-f3d7546dee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f31ef67-5164-47f6-99c4-46252641f6d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Benefits and When to Use\n",
    "\n",
    "1. **Performance Improvement:** Parallelizing code, especially on accelerators like GPUs, can lead to significant speedups. Operations like matrix multiplications, which can be heavily parallelized, benefit greatly.\n",
    "\n",
    "2. **Memory Efficiency:** For large-scale computations, distributing data and computations across multiple devices can help in tackling memory limitations of a single device.\n",
    "\n",
    "However, one should be aware of the communication overhead between devices. If the devices need to communicate frequently or exchange large amounts of data, the benefits of parallelization might be diminished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0c8ef-c673-4902-9a93-e51e9c712b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jax_coding_club]",
   "language": "python",
   "name": "conda-env-jax_coding_club-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
