{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc099f4-b49f-418c-8c72-5e5622618363",
   "metadata": {},
   "source": [
    "# Introduction to JAX\n",
    "\n",
    "At its core, JAX provides a way to transform functions using NumPy-like syntax to get automatic differentiation, GPU/TPU acceleration, and much more.\n",
    "\n",
    "## JAX and Numpy\n",
    "\n",
    "Using JAX feels very much like using NumPy. The majority of your NumPy knowledge is directly transferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2462ea-955b-44b8-a6dc-fe44042955e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 12:09:24.015324: W external/xla/xla/service/gpu/nvptx_compiler.cc:673] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.3.52). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# basic examples: array creation, reshaping, and slicing in both JAX and NumPy as side-by-side code snippets.\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Examples of array definition:\n",
    "# Numpy\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "numpy_ones = np.ones(10)\n",
    "\n",
    "# JAX\n",
    "jax_array = jnp.array([[1, 2], [3, 4]])\n",
    "jax_ones = jax.numpy.ones(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1202727-543d-4eb2-9045-cb452e433662",
   "metadata": {},
   "source": [
    "However, JAX arrays are diffenrent in several important ways: \n",
    "\n",
    "- **Immutability:** Once an array is created, its contents cannot be changed, the same way python `tuple` works. \n",
    "- **Device Memory:** JAX arrays can live in device memory (like on a GPU), while NumPy arrays live in host memory.\n",
    "- **Lazy Evaluation:** Unlike NumPy which evaluates operations immediately, JAX can be lazy in its evaluation, waiting to execute operations until the result is actually needed. This is especially true when using some of JAX's transformations. (Hard to showcase with a simple example).\n",
    "- **Advanced Features:** JAX arrays operations can be auto-differentiated and parallelized (we touch on this later).\n",
    "\n",
    "## Immutability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91addb37-ab41-42f2-af58-6f382f4a2fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exeption raised: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    }
   ],
   "source": [
    "# Array immutability\n",
    "\n",
    "# Item assignment works for numpy arrays.\n",
    "numpy_array[1, 1] = 0\n",
    "\n",
    "# While in JAX it raises an TypeError.\n",
    "try:\n",
    "    jax_array[1, 1] = 0\n",
    "except TypeError as err:\n",
    "    print(f\"Exeption raised: {err}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317be93-2a6d-40cc-868b-7c2beb322b67",
   "metadata": {},
   "source": [
    "## Device Memory\n",
    "\n",
    "This is how you can display the device in which the array is on, and send it to another device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38689f57-79d5-4a58-a49d-d7dc0c2bdab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array device: cuda:0\n",
      "List available devices [cuda(id=0)]\n",
      "Array device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "# Display the device the array is on\n",
    "print(\"Array device:\", jax_array.device_buffer.device())\n",
    "\n",
    "# List all available devices\n",
    "print(\"List available devices\", jax.devices())\n",
    "\n",
    "# Send array to a desired device\n",
    "jax_array = jax.device_put(jax_array, jax.devices(\"cpu\")[0])\n",
    "print(\"Array device:\", jax_array.device_buffer.device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94681d7-5416-400c-b1d8-5026bda302d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPU-acceleration \n",
    "As a simple use case, we can see that on large matrices, JAX's GPU acceleration can be noticeably faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badc7652-8ad2-4043-b592-bc08780bde60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 ms ± 18.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "19 ms ± 847 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Create large matrices\n",
    "large_numpy_matrix = np.random.rand(5000, 5000)\n",
    "large_jax_matrix = jnp.array(large_numpy_matrix)\n",
    "\n",
    "# Time matrix multiplication in NumPy\n",
    "%timeit np.dot(large_numpy_matrix, large_numpy_matrix)\n",
    "\n",
    "# Time matrix multiplication in JAX\n",
    "%timeit jnp.dot(large_jax_matrix, large_jax_matrix).block_until_ready() # The block_until_ready ensures we wait for the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc55bd6-63be-4059-a149-31cdd6e9439a",
   "metadata": {},
   "source": [
    "## Just-In-Time Compilation with `jit`\n",
    "\n",
    "### What is JIT Compilation?\n",
    "\n",
    "Just-In-Time (JIT) Compilation is a technique where the code is compiled during execution rather than before execution starts. This allows certain optimizations to be made that can't be achieved with traditional ahead-of-time compilation.\n",
    "\n",
    "For those of you familiar with `numba`, JAX's `jit` serves a similar purpose: it takes a Python function and optimizes it for faster execution. However, while `numba` is designed primarily for speeding up CPU-bound Python code, JAX's `jit` can target both CPU and accelerators like GPUs and TPUs.\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Here's how to use `jit` in JAX:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada960c8-6322-4239-aa42-2f1cec9570b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4546487  -0.37840125 -0.13970774]\n",
      "JIT compiled with decorators: <PjitFunction of <function decorated_function at 0x7f4c3947c670>>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def slow_function(x):\n",
    "    return jnp.sin(x) * jnp.cos(x)\n",
    "\n",
    "compiled_function = jax.jit(slow_function)\n",
    "\n",
    "# Now, fast_function will run the optimized version\n",
    "result = compiled_function(jnp.array([1.0, 2.0, 3.0]))\n",
    "print(result)\n",
    "\n",
    "# Alternatively you can use decorators\n",
    "@jax.jit\n",
    "def decorated_function(x):\n",
    "    return jnp.sin(x) * jnp.cos(x)\n",
    "\n",
    "print(f\"JIT compiled with decorators: {decorated_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64423472-ad71-4a20-b2c9-2b6610ee33ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tracers\n",
    "When you jit a function, JAX introduces what are known as \"tracers\" into the function. Tracers can be thought of as symbolic representations of your input, and they help JAX figure out how to optimize your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbcc886-e7e2-40ad-aac8-0b5b23e52650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[3]. let\n",
      "    b:f32[3] = sin a\n",
      "    c:f32[3] = cos a\n",
      "    d:f32[3] = mul b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "from jax import make_jaxpr # visualize the series of operations (expressed as a JAX expression or \"jaxpr\")\n",
    "\n",
    "# Let's see the jax expression of our function\n",
    "print(make_jaxpr(slow_function)(jnp.array([1.0, 2.0, 3.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab800a1b-5a50-4334-bce6-3db44d801abc",
   "metadata": {},
   "source": [
    "The output is a JAX expression, a symbolic representation of your function's operations. This is what JAX uses internally to optimize your code.\n",
    "\n",
    "### Debugging JIT-compiled Code\n",
    "\n",
    "Certain functions, such as `jax.lax.scan`, undergo JIT compilation by default. This can make debugging challenging since, during the process, you'll be able to inspect tracers rather than the computed values. To facilitate debugging, you can deactivate the jit compilation as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b6ce62-0bb9-4a08-b2fb-ab3a4a8815c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT enabled\n",
      "Iteration 1:\n",
      "Inside jitted function: Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Iteration 2:\n",
      "Iteration 3:\n",
      "\n",
      "JIT disabled\n",
      "Iteration 1:\n",
      "Inside jitted function: [1. 2. 3.]\n",
      "Iteration 2:\n",
      "Inside jitted function: [1. 2. 3.]\n",
      "Iteration 3:\n",
      "Inside jitted function: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@jax.jit\n",
    "def jitted_function(x):\n",
    "    print(\"Inside jitted function:\", x)\n",
    "\n",
    "# Call the function with some data\n",
    "# The print inside the function will be executed only once, at compilation time\n",
    "print(\"JIT enabled\")\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "for i in range(3):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    jitted_function(x)\n",
    "\n",
    "print(\"\\nJIT disabled\")\n",
    "with jax.disable_jit():\n",
    "    for i in range(3):\n",
    "        print(f\"Iteration {i+1}:\")\n",
    "        jitted_function(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5424ce3-86c2-4844-a046-896aa32f3d2d",
   "metadata": {},
   "source": [
    "## Autodifferentiation with JAX\n",
    "Autodifferentiation, often referred to as \"autograd\" or simply \"AD\", is the ability to compute gradients of functions automatically. JAX provides this capability seamlessly. Let's take a look at how this works.\n",
    "\n",
    "### Toy Example 1: Linear Regression\n",
    "We can start off with a standard example, fitting a linear regression by gradient descent on a Mean Squared Error loss.\n",
    "\n",
    "Let's consider a simple linear regression model: $f(x)=mx+b$, and $y=f(x) + \\text{noise}$, where we'll optimize for the slope $m$ and the intercept $b$. We'll use the Mean Squared Error as our loss function.\n",
    "\n",
    "First, let's generate some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f1ba9b-9a1a-4f92-b8bc-79639da98743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "# Generating some sample data\n",
    "true_m = 2.5\n",
    "true_b = -1.0\n",
    "x_data = jnp.linspace(-10, 10, 1000)\n",
    "y_data = true_m * x_data + true_b + jax.random.normal(jax.random.PRNGKey(123), shape=x_data.shape) * 2.0  # Adding some noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d655e01-2c39-49db-99b9-7b4b1d365772",
   "metadata": {},
   "source": [
    "Let's define the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d293e70-c0d7-49cc-b339-e7a00486bf65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "def mse_loss(params, x, y):\n",
    "    m, b = params\n",
    "    y_pred = linear_model(x, m, b)\n",
    "    return np.mean((y_pred - y)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dcb3eb-34a5-4d92-8acb-e1528978ddeb",
   "metadata": {},
   "source": [
    "We can now compute the gradient via AD, by passing the lossn through the `jax.grad` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49d1a87d-4c47-4c30-b99d-fae89a623b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-166.60902  ,    1.7543185], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = jax.grad(mse_loss, argnums=0)  # 0 refers to the first argument of mse_loss, i.e., the params tuple\n",
    "gradient(jnp.array([0.0, 0.0]), x_data,y_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f2b7d-66a4-400a-8c68-9e07ef273e5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "One can perform gradien decent or any other gradient based optimization on the loss. \n",
    "\n",
    "There are many excellent libraries for optimization in the `jax` framwork, for example [`JAXopt`](https://jaxopt.github.io/stable/) for general purpose optimizaiton or [`flax`](https://flax.readthedocs.io/en/latest/) which is specific for neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e962562-130e-4621-a3a7-bb8e653e4210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92: m = 2.4941, b = -0.7404, Loss = 4.1020\n",
      "Iteration 93: m = 2.4941, b = -0.7432, Loss = 4.1012\n",
      "Iteration 94: m = 2.4941, b = -0.7458, Loss = 4.1005\n",
      "Iteration 95: m = 2.4941, b = -0.7485, Loss = 4.0998\n",
      "Iteration 96: m = 2.4941, b = -0.7510, Loss = 4.0992\n",
      "Iteration 97: m = 2.4941, b = -0.7536, Loss = 4.0985\n",
      "Iteration 98: m = 2.4941, b = -0.7560, Loss = 4.0979\n",
      "Iteration 99: m = 2.4941, b = -0.7585, Loss = 4.0974\n",
      "Iteration 100: m = 2.4941, b = -0.7608, Loss = 4.0968\n",
      "\n",
      "Optimization results,  m: 2.4941420555114746, b: -0.7608301043510437\n",
      "JAXopt results,        m: 2.4940860271453857, b: -0.8771581649780273\n"
     ]
    }
   ],
   "source": [
    "import jaxopt\n",
    "\n",
    "# Gradient Descent\n",
    "learning_rate = 0.01\n",
    "params = jnp.array([0.0, 0.0])  # Initial values for m and b\n",
    "\n",
    "for i in range(100):  # 100 gradient descent updates\n",
    "    grad_params = gradient(params, x_data, y_data)\n",
    "    params -= learning_rate * grad_params\n",
    "    if i > 90:\n",
    "        print(f\"Iteration {i+1}: m = {params[0]:.4f}, b = {params[1]:.4f}, Loss = {mse_loss(params, x_data, y_data):.4f}\")\n",
    "print(f\"\\nOptimization results,  m: {params[0]}, b: {params[1]}\")\n",
    "      \n",
    "# Gradient Descent via JAXopt\n",
    "solver = jaxopt.GradientDescent(fun=mse_loss, tol=10**-6)\n",
    "params, state = solver.run(init_params=(1., 1.), x=x_data, y=y_data)\n",
    "print(f\"JAXopt results,        m: {params[0]}, b: {params[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a647b5e-ec32-4177-952b-5e62fafff3a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Toy Example 2: Regression with a Two-layer Neural Network\n",
    "Now, let's add a two feed-forward layer with a ReLU activation. \n",
    "\n",
    "This will transform our model from a simple linear regression to a slighty complex neural network. The beauty of autodiff, is that this can be handled in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08cfc658-84c9-412b-bdd3-5d50b427373d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 212.3490\n",
      "Iteration 2: Loss = 212.1717\n",
      "Iteration 3: Loss = 212.0583\n",
      "Iteration 4: Loss = 211.9857\n",
      "Iteration 5: Loss = 211.9392\n",
      "Iteration 6: Loss = 211.9094\n",
      "Iteration 7: Loss = 211.8904\n",
      "Iteration 8: Loss = 211.8782\n",
      "Iteration 9: Loss = 211.8704\n",
      "Iteration 10: Loss = 211.8654\n"
     ]
    }
   ],
   "source": [
    "def two_layer_nn_model(x, w1, b1, w2, b2, w_out, b_out):\n",
    "    hidden1 = jax.nn.relu(jnp.dot(x, w1) + b1)  # First ReLU activation\n",
    "    hidden2 = jax.nn.relu(jnp.dot(hidden1, w2) + b2)  # Second ReLU activation\n",
    "    return jnp.dot(hidden2, w_out) + b_out\n",
    "\n",
    "def two_layer_nn_mse_loss(params, x, y):\n",
    "    w1, b1, w2, b2, w_out, b_out = params\n",
    "    y_pred = two_layer_nn_model(x, w1, b1, w2, b2, w_out, b_out)\n",
    "    return jnp.mean((y_pred - y)**2)\n",
    "\n",
    "gradient_two_layer_nn = jax.grad(two_layer_nn_mse_loss, argnums=0)\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "params_two_layer_nn = (jnp.zeros((1, 10)), jnp.zeros(10), jnp.zeros((10, 10)), jnp.zeros(10), jnp.zeros(10), 0.0)  # Initialize weights and biases\n",
    "\n",
    "for i in range(100):\n",
    "    grad_params_two_layer_nn = gradient_two_layer_nn(params_two_layer_nn, x_data[:, None], y_data)  # x_data[:, None] reshapes x_data for matrix multiplication\n",
    "    params_two_layer_nn = tuple(param - learning_rate * grad_param for param, grad_param in zip(params_two_layer_nn, grad_params_two_layer_nn))\n",
    "    if i < 10:\n",
    "        print(f\"Iteration {i+1}: Loss = {two_layer_nn_mse_loss(params_two_layer_nn, x_data[:, None], y_data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee40a1d-bcd6-4c3d-9701-8a767f6d9f40",
   "metadata": {},
   "source": [
    "## Vectorization and Parallelization in JAX: `vmap` and `pmap`\n",
    "\n",
    "Parallelization is the process of performing multiple computations simultaneously. JAX offers two primary mechanisms for parallelizing code:\n",
    "\n",
    "1. Vectorization with **`vmap`**: This is a way to transparently turn a function that operates on single data points into one that operates on batches of data points.\n",
    "2. Parallelization across devices with **`pmap`**: It allows you to distribute computations over multiple accelerators.\n",
    "\n",
    "### Vectorization with vmap\n",
    "\n",
    "Suppose you have a function that computes the square of a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef32e29-ce52-48df-b3f4-d833c899b9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2520c4-ab38-4125-b1d6-7e3f0c863053",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you want to compute the square of a batch of numbers, one way is to use a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1c9d6c0-3deb-42bb-b44b-92a33b468850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "squared = jnp.array([square(x) for x in xs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c01a4-d332-44cd-8845-a816cef2d61a",
   "metadata": {},
   "source": [
    "With **`vmap`**, you can vectorize the square function so it can process the entire batch at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d19c211-951d-4f1c-b624-8d7c4b50ca0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batched_square = jax.vmap(square)\n",
    "squared = batched_square(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed1cf6-5723-461f-a665-cdc3222889ab",
   "metadata": {},
   "source": [
    "And parallelizaiton can be done on mutlidimensional arrays, specfing the input and output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847811b2-c1e2-42c0-b66c-140860b22c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare outputs:\n",
      "vmap:\n",
      " [[ 7 10]\n",
      " [15 22]] \n",
      "regular dot product:\n",
      " [[ 7 10]\n",
      " [15 22]]\n",
      "\n",
      "\n",
      "Transpose outputs:\n",
      " [[ 7 15]\n",
      " [10 22]]\n"
     ]
    }
   ],
   "source": [
    "# Vector doct product\n",
    "vector_vector = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -> []\n",
    "\n",
    "# Using vmap, upgrade vv to handle matrix-vector multiplication. \n",
    "# We parallelize across the rows of the matrix (axis 0) and broadcast the vector.\n",
    "matrix_vector = jax.vmap(vector_vector, in_axes=(0, None), out_axes=0)      #  ([b,a], [a]) -> [b]      (b is the mapped axis)\n",
    "\n",
    "# Apply vmap again to upgrade mv to handle matrix-matrix multiplication. \n",
    "# This time we parallelize across the columns of the second matrix (axis 1).\n",
    "matrix_matrix = jax.vmap(matrix_vector, in_axes=(None, 1), out_axes=1)      #  ([b,a], [a,c]) -> [b,c]  (c is the mapped axis)\n",
    "\n",
    "print(\"Compare outputs:\\nvmap:\\n\", matrix_matrix(jax_array, jax_array), \"\\nregular dot product:\\n\", jnp.dot(jax_array, jax_array))\n",
    "\n",
    "# The out_axes argument specifies where to store the parallelized axis in the output. \n",
    "# By setting it to 0 in the next function, we effectively transpose the output of the matrix-matrix multiplication.\n",
    "matrix_matrix_transpose = jax.vmap(matrix_vector, in_axes=(None, 1), out_axes=0)      #  ([b,a], [a,c]) -> [c,b]  (c is the mapped axis)\n",
    "print(\"\\n\\nTranspose outputs:\\n\", matrix_matrix_transpose(jax_array, jax_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179764d-acf4-4ed4-9fc1-6fe181c8875c",
   "metadata": {},
   "source": [
    "### Parallelization across devices with **`pmap`**\n",
    "`pmap` (parallel map) is a parallelized version of `vmap` and is meant to distribute computations across multiple devices, typically multiple GPUs or TPUs. While vmap vectorizes computations, pmap physically runs them in parallel across devices. Here's a way to build on top of the matrix multiplication example to introduce pmap.\n",
    "1. **Setup**\n",
    "Before using pmap, you need to check the available devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "890fd8a1-cecf-4773-9d2e-749938d99a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# assume that you have multiple gpus\n",
    "devices = jax.devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdc5a0-0cd1-4b42-a94b-d65b84ba66e4",
   "metadata": {},
   "source": [
    "**NOTE** You should take the result with a grain of salt. In real-world scenarios, GPU computation is often magnitudes faster than CPU for tasks like matrix multiplication. Plus, the data transfer time between the host (CPU) and device (GPU) can be a significant overhead. So, while it's an interesting demonstration, this setup might not be practically efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "828fcbfc-705f-4f30-ade0-7dd9bac7ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 10)\n",
      "Device: cuda:0\n",
      "Shape: (10, 10)\n",
      "Device: cuda:0\n",
      "Result Shape: (2, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create two arrays with leading dimension 2, intended for distribution across two devices.\n",
    "# Each sub-array (of shape (10, 10)) will be sent to a separate device.\n",
    "x_sharded = jax.random.normal(jax.random.PRNGKey(123), shape=(2, 10, 10))\n",
    "y_sharded = jax.random.normal(jax.random.PRNGKey(456), shape=(2, 10, 10))  # Note: Using a different seed for variety.\n",
    "\n",
    "# Define a parallelized matrix multiplication operation using `pmap`.\n",
    "# Note: We are specifying devices to make sure the operation runs in parallel across the intended devices.\n",
    "# Here, we duplicate the devices list, assuming `devices` originally has one device for CPU and one for GPU.\n",
    "pmap_multiply = jax.pmap(matrix_matrix, devices=devices*2)\n",
    "\n",
    "# Execute the parallel matrix multiplication. Each pair of sub-arrays from x_sharded and y_sharded \n",
    "# will be multiplied on a separate device.\n",
    "result = pmap_multiply(x_sharded, y_sharded)\n",
    "\n",
    "# Explore the result buffers, which provides details about the shape of the result \n",
    "# and the device on which each computation was performed.\n",
    "for buffer in result.device_buffers:\n",
    "    print(\"Shape:\", buffer.shape)\n",
    "    print(\"Device:\", buffer.device_buffer.device())\n",
    "\n",
    "# Print the shape of the `result` to demonstrate that, to the end user,\n",
    "# the result appears as a standard array, even though it was computed in a parallelized manner.\n",
    "print(\"Result Shape:\", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31ef67-5164-47f6-99c4-46252641f6d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Benefits and When to Use\n",
    "\n",
    "1. **Performance Improvement:** Parallelizing code, especially on accelerators like GPUs, can lead to significant speedups. Operations like matrix multiplications, which can be heavily parallelized, benefit greatly.\n",
    "\n",
    "2. **Memory Efficiency:** For large-scale computations, distributing data and computations across multiple devices can help in tackling memory limitations of a single device.\n",
    "\n",
    "However, one should be aware of the communication overhead between devices. If the devices need to communicate frequently or exchange large amounts of data, the benefits of parallelization might be diminished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea6a9c-c346-417f-84dc-43c49fc0a3a3",
   "metadata": {},
   "source": [
    "## Pytrees\n",
    "\n",
    "A \"pytree\" is a tree of Python containers (e.g., lists, tuples, dicts) that can contain numerical arrays (like `numpy.ndarray` or `jax.numpy.DeviceArray`) as leaves. In the context of JAX, pytrees are useful because many JAX operations that take array arguments also support pytree arguments, allowing you to build and manipulate more complex structures.\n",
    "For a more comprehensive intro see the [official documentation](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html), which is very good.\n",
    "\n",
    "### Basic Pytree Examples\n",
    "\n",
    "Here are some examples of pytrees:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759395c6-58a1-46c9-af16-5818cfebf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single array\n",
    "leaf = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# A tuple of arrays\n",
    "tree = (jnp.array([1.0, 2.0, 3.0]), jnp.array([4.0, 5.0]))\n",
    "\n",
    "# A dictionary of arrays\n",
    "tree = {\"a\": jnp.array([1.0, 2.0, 3.0]), \"b\": jnp.array([4.0, 5.0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b23a78-9674-441e-8d1a-e9daf4988f1c",
   "metadata": {},
   "source": [
    "### Tree Utilities\n",
    "\n",
    "JAX provides utilities to manipulate pytrees:\n",
    "\n",
    "- **tree_map**: applies a function to each leaf in a pytree or multiple pytrees.\n",
    "- **tree_reduce**: aggregates values across all the leaves of a pytree.\n",
    "\n",
    "Many others can be found in the `jax.tree_util` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84ad5b8-12c2-469e-a03f-527f49bd360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': Array([2., 4.], dtype=float32), 'b': (Array([6., 8.], dtype=float32), Array([10.], dtype=float32))}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tree = {\"a\": jnp.array([1.0, 2.0]), \"b\": (jnp.array([3.0, 4.0]), jnp.array([5.0]))}\n",
    "result = jax.tree_map(lambda x: x * 2, tree)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b04747-f954-4a0a-a745-8ef6fbd9de63",
   "metadata": {},
   "source": [
    "### Why are Pytrees Useful in JAX?\n",
    "\n",
    "When defining models or optimization routines in JAX, it's common to have parameters structured in nested containers, especially when using neural networks libraries  like Flax or Haiku. Pytrees allow for easy manipulation and transformation of these structures without having to manually manage each individual array.\n",
    "\n",
    "Understanding how to work with pytrees helps in:\n",
    "\n",
    "- Building more complex models with structured parameters.\n",
    "- Writing generic JAX code that works seamlessly with these complex structures.\n",
    "- Utilizing JAX transformations like grad on functions that accept or return these structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f877b-3eb6-4856-ada0-bd436e335868",
   "metadata": {},
   "source": [
    "### Pytrees and Neural Networks\n",
    "\n",
    "When you're working with neural networks, the weights and biases are typically stored as arrays. As the network grows in complexity, managing these arrays individually can become cumbersome. Pytrees provide a more structured way to handle these parameters, making the code more readable and easier to maintain.\n",
    "\n",
    "### Two-Layer Neural Network Revisited with Pytrees\n",
    "\n",
    "First, we define our network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dd08d18-1e20-4056-891e-0de1a4767db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(input_dim, hidden_dim, output_dim):\n",
    "    params = {\n",
    "        \"layer1\": {\n",
    "            \"weights\": jax.random.normal(jax.random.PRNGKey(123), shape=(input_dim, hidden_dim)),\n",
    "            \"biases\": jnp.zeros(hidden_dim)\n",
    "        },\n",
    "        \"layer2\": {\n",
    "            \"weights\": jax.random.normal(jax.random.PRNGKey(246), shape=(hidden_dim, output_dim)),\n",
    "            \"biases\": jnp.zeros(output_dim)\n",
    "        }\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330de803-8b5c-4f20-b802-2703662e0659",
   "metadata": {},
   "source": [
    "Notice how we're using nested dictionaries to represent the layers and their respective weights and biases.\n",
    "\n",
    "Next, our forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "595953f2-1fbe-4878-accc-a45a9fcf6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, x):\n",
    "    # First layer\n",
    "    z1 = jnp.dot(x, params[\"layer1\"][\"weights\"]) + params[\"layer1\"][\"biases\"]\n",
    "    a1 = jax.nn.relu(z1)\n",
    "    \n",
    "    # Second layer\n",
    "    z2 = jnp.dot(a1, params[\"layer2\"][\"weights\"]) + params[\"layer2\"][\"biases\"]\n",
    "    a2 = jax.nn.relu(z2)\n",
    "    \n",
    "    return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a82a9-807c-4c05-9f80-80f185d258b9",
   "metadata": {},
   "source": [
    "### Advantages of Pytrees and Tree Operations\n",
    "\n",
    "1. **Structured Representation**: Pytrees provide a clear and organized representation of model parameters, especially for models with multiple layers or components.\n",
    "\n",
    "2. **Simplified Parameter Updates**: With the tree utilities provided by JAX, operations like updating model parameters after a gradient descent step become more streamlined.\n",
    "\n",
    "3. **Scalability**: As your model grows, pytrees keep the code maintainable. Whether you have two layers or twenty, the structure remains consistent.\n",
    "\n",
    "4. **Generic Code**: You can write more general-purpose functions that operate over model parameters, without having to know the exact structure or layout of the model. For example, you can write a generic training loop for any model, as long as the parameters are structured as pytrees.\n",
    "\n",
    "5. **Compatible with JAX Transformations**: Pytrees work seamlessly with JAX functions like `grad`, `jit`, and others. When you compute the gradient of a function with respect to a pytree of parameters, JAX returns the gradients in the same pytree structure.\n",
    "\n",
    "### Example: Gradient Descent Update with Pytrees\n",
    "\n",
    "Using JAX's tree utilities, updating the parameters of the network after computing gradients becomes straightforward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74d40e2-3a27-4367-ad90-ee1d71a1f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error loss\n",
    "def loss(params, x, y_true):\n",
    "    y_pred = forward(params, x)\n",
    "    return jnp.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Compute gradients of the loss with respect to the parameters\n",
    "grad_fn = jax.grad(loss)\n",
    "\n",
    "def gradient_descent_step(params, x, y_true, learning_rate=0.01):\n",
    "    gradients = grad_fn(params, x, y_true)\n",
    "    return jax.tree_map(lambda p, g: p - learning_rate * g, params, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7de93-d2b8-49e7-acdc-37d2686615b0",
   "metadata": {},
   "source": [
    "Here, `tree_map` is used to subtract the computed gradients from the current parameters, effectively implementing a gradient descent update. The code remains clean and general, regardless of the specific structure of the `params` pytree.\n",
    "\n",
    "In summary, pytrees and tree operations make handling the parameters of neural networks (or any machine learning model) in JAX more organized, flexible, and scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1adb56dd-e03e-492f-86bf-8a1fce854889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 -> biases: (10,)\n",
      "layer1 -> weights: (1, 10)\n",
      "layer2 -> biases: (1,)\n",
      "layer2 -> weights: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "def print_leaf_shapes(d, path=[]):\n",
    "    for k, v in d.items():\n",
    "        new_path = path + [k]\n",
    "        if isinstance(v, dict):\n",
    "            print_leaf_shapes(v, new_path)\n",
    "        else:\n",
    "            # Assuming the leaf is a JAX array; replace with appropriate check if not\n",
    "            print(f\"{' -> '.join(new_path)}: {v.shape}\")\n",
    "\n",
    "\n",
    "update = gradient_descent_step(init_network(1, 10, 1), x_data[:, None], y_data[:, None])\n",
    "\n",
    "# returns a tree with the same srtucture of the input parameters.\n",
    "print_leaf_shapes(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50e18b-ebc4-43cc-8201-53441c129e83",
   "metadata": {},
   "source": [
    "Here, `tree_multimap` is used to subtract the computed gradients from the current parameters, effectively implementing a gradient descent update. The code remains clean and general, regardless of the specific structure of the `params` pytree.\n",
    "\n",
    "In summary, pytrees and tree operations make handling the parameters of neural networks (or any machine learning model) in JAX more organized, flexible, and scalable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051d79-553c-4308-a8d8-df27288d7bcd",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [**Awesome JAX**](https://github.com/n2cholas/awesome-jax) a curated list of `JAX`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c136a-58f5-4464-9ec4-d796a9b2afda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c687a-9afd-4678-b705-d5d385a8c4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
